{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Now, this movie is the worst i have ever seen!...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Slow, boring, extremely repetitive. No wonder ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>For those fans of Laurel and Hardy, the 1940s ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>First things first - though I believe Joel Sch...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>This is one gripping movie, reminiscent of the...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  sentiment\n",
       "0  Now, this movie is the worst i have ever seen!...          0\n",
       "1  Slow, boring, extremely repetitive. No wonder ...          0\n",
       "2  For those fans of Laurel and Hardy, the 1940s ...          0\n",
       "3  First things first - though I believe Joel Sch...          0\n",
       "4  This is one gripping movie, reminiscent of the...          1"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('shuffled_movie_data.csv')\n",
    "df.head()\n",
    "#df.iloc[0]['review']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11841</th>\n",
       "      <td>I have seen this movie and in all honestly was...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19602</th>\n",
       "      <td>I only saw this recently but had been aware of...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45519</th>\n",
       "      <td>Albert Pyun delivers a very good action/drama ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25747</th>\n",
       "      <td>This is a truly great and beautiful movie. The...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42642</th>\n",
       "      <td>You know, I'm sure the boys were sitting aroun...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  review  sentiment\n",
       "11841  I have seen this movie and in all honestly was...          0\n",
       "19602  I only saw this recently but had been aware of...          1\n",
       "45519  Albert Pyun delivers a very good action/drama ...          1\n",
       "25747  This is a truly great and beautiful movie. The...          1\n",
       "42642  You know, I'm sure the boys were sitting aroun...          0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "#Aleatorizamos las entradas\n",
    "np.random.seed(0)\n",
    "df = df.reindex(np.random.permutation(df.index))\n",
    "df[['review','sentiment']].to_csv('shuffled_movie_data.csv',index=False)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocesamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop = stopwords.words('english')\n",
    "porter = PorterStemmer()\n",
    "\n",
    "def tokenizer(text):\n",
    "    text = re.sub('<[^>]*>', '', text)\n",
    "    emoticons = re.findall('(?::|;|=)(?:-)?(?:\\)|\\(|D|P)', text.lower())\n",
    "    text = re.sub('[\\W]+', ' ', text.lower()) + ' '.join(emoticons).replace('-', '')\n",
    "    #print(text.split())\n",
    "    text = [w for w in text.split() if w not in stop]\n",
    "    \n",
    "    tokenized = [porter.stem(w) for w in text]\n",
    "    return text\n",
    "    #return ['good','nice','bad',\"don't like\"]\n",
    "#tokenizer(df.iloc[1]['review'])\n",
    "#print(type(tx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prueba del tokenizer\n",
    "# tokenizer(df.iloc[5]['review'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stream_docs(path):\n",
    "    with open(path,'r') as csv:\n",
    "        next(csv)\n",
    "        for line in csv:\n",
    "            text,label = line[:-3],int(line[-2])\n",
    "            yield text, label\n",
    "\n",
    "#stream = stream_docs('shuffled_movie_data.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#next(stream_docs(path='shuffled_movie_data.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_minibatch(doc_stream, size):\n",
    "    docs, y = [], []\n",
    "    for _ in range(size):\n",
    "        text, label = next(doc_stream)\n",
    "        docs.append(text)\n",
    "        y.append(label)\n",
    "    return docs, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EJERCICIO 1\n",
    "\n",
    "Agregar nuevos Features\n",
    "\n",
    "Según https://web.stanford.edu/~jurafsky/slp3/7.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing Opinions\n",
      "Completed  0.0 %\n",
      "Completed  20.0 %\n",
      "Completed  40.0 %\n",
      "Completed  60.0 %\n",
      "Completed  80.0 %\n",
      "Adding up all values\n",
      "Completed  20.0 %\n",
      "Completed  40.0 %\n",
      "Completed  60.0 %\n",
      "Completed  80.0 %\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "clf = SGDClassifier(loss='log', random_state=1, n_iter=1)\n",
    "doc_stream = stream_docs(path='shuffled_movie_data.csv')\n",
    "\n",
    "from nltk.util import ngrams\n",
    "from nltk import FreqDist\n",
    "\n",
    "data = [\"this\", \"is\", \"not\", \"a\", \"test\", \"this\", \"is\", \"real\", \"not\", \"a\", \"test\", \"this\", \"is\", \"this\", \"is\", \"real\", \"not\", \"a\", \"test\"]\n",
    "\n",
    "from scipy.stats import itemfreq\n",
    "\n",
    "\n",
    "dict = {}\n",
    "for size in 1,2,3:\n",
    "    dict[size] = FreqDist(ngrams(data,size))\n",
    "    \n",
    "l =[]\n",
    "\n",
    "for values in dict.values():\n",
    "    l += list(values.values())\n",
    "\n",
    "##PASOS PARA GENERAR EL ALGORITMO\n",
    "###TOKENIZAR CADA PARRAFO DE ENTRENAMIENTO\n",
    "#####HACER LA TOKENIZACION CON 1-, 2-, Y 3- GRAMS\n",
    "###ENCONTRAR LAS FRENCUENCIAS DE DISTRIBUCION PARA CADA TOKEN\n",
    "\n",
    "### SUMAR LOS TOKENS IGUALES EN TODOS LOS DATOS DE ENTRAMIENTO\n",
    "### ORDENARLOS Y TRABAJAR CON SOLO LOS N MAYORES\n",
    "\n",
    "### CREAR UN VECTOR DE PESOS ALEATORIOS ENTRE 0-1 Y EMPEZAR EL \n",
    "### EVALUAR\n",
    "\n",
    "### CREAR EL ALGORITMO STOCHASTIC GRANDIENT ASCENT Y EMPEZAR\n",
    "### EL PROCESAMIENTO CON BATCHS DE 1000 O MAS\n",
    "### ITERAR SOBRE LOS 45000 DATOS DE ENTRENAMIENTO\n",
    "\n",
    "### ETAPA DE PRUEBAS\n",
    "### PROCESAR LOS DATOS RESTANTES Y EVALUAR LA CANTIDAD DE\n",
    "### ACIERTOS EXPRESANDO EL RESULTADO EN FORMA DE PORCENTAJE\n",
    "\n",
    "### PARA EL EJERCICIO 1: TAN SOLO VARIAR LOS TOKENS, Y AGREGAR\n",
    "### UNOS CUANTOS NUEVOS\n",
    "\n",
    "\n",
    "##### ENCONTRANDO LAS PALABRAS MAS USADAS EN TODO EL CORPUS\n",
    "##### USAMOS LA FUNCION TOKENIZER QUE FILTRA STOPWORDS\n",
    "\n",
    "\n",
    "\n",
    "#tokenizer(df.iloc[0]['review'])\n",
    "\n",
    "n = 50000 #numero de ejemplos muestrados\n",
    "print(\"Tokenizing Opinions\")\n",
    "corpus_words = [] ## Examples tokenized\n",
    "for i in range(n):\n",
    "    corpus_words += [tokenizer(df.iloc[i]['review'])]\n",
    "    if i %10000 == 0:\n",
    "        print(\"Completed \",20*i/10000,\"%\")\n",
    "#print(corpus_words)\n",
    "\n",
    "word_dict = {}\n",
    "for size in 1,2,3,4,5:\n",
    "    word_dict[size] =  FreqDist(ngrams(corpus_words[0],size))\n",
    "\n",
    "print(\"Adding up all values\")\n",
    "for i in range(1,n):\n",
    "    for size in 1,2,3,4,5:\n",
    "        word_dict[size].update(FreqDist(ngrams(corpus_words[i],size)))\n",
    "    if i %10000 == 0:\n",
    "        print(\"Completed \",20*i/10000,\"%\")\n",
    "\n",
    "#for i in 1,2,3,4,5:\n",
    "#    print(word_dict[i].most_common(30))\n",
    "    \n",
    "#Supongamos que nuestras características son los 10 ngramas (n[1,5])\n",
    "#mas frecuentes en toda la base de datos\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300\n"
     ]
    }
   ],
   "source": [
    "#word_dict[3].most_common(2)[0][0]\n",
    "\n",
    "## Aca debemos contar la ocurrencia de los n gramas obtenidos y guardarlos\n",
    "## para formar un vector para cada dato de ejemplo\n",
    "\n",
    "number_features = 0\n",
    "keys = []\n",
    "for i in 1,2,3,4,5:\n",
    "    number_features+= 90-10*i\n",
    "    tmp = word_dict[i].most_common(90-10*i)\n",
    "    for j in range(90-10*i):\n",
    "        keys += (tmp[j][0],)\n",
    "#keys\n",
    "#number_features = 300!\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#print(keys)\n",
    "def get_vectors(_start,_stop,number_features):\n",
    "    text_tmp_dict = {}\n",
    "    for i in range(number_features):\n",
    "        text_tmp_dict[keys[i]] = i\n",
    "    #print(text_tmp_dict)    \n",
    "    \n",
    "    trainingSet_length = _stop - _start\n",
    "    trainingSet = []\n",
    "    for example in range(_start,_stop):\n",
    "        text_tmp = corpus_words[example]\n",
    "        #print(text_tmp)\n",
    "        #print(\"Occurency in Example\")\n",
    "\n",
    "        word_vector = [0 for i in range(number_features)]\n",
    "        #print(word_vector)\n",
    "\n",
    "        for i in 1,2,3,4,5:\n",
    "            for token in ngrams(text_tmp,i):\n",
    "                if token in keys:\n",
    "                    #print(token)\n",
    "                    word_vector[text_tmp_dict[token]] += 1\n",
    "                    #text_tmp_dict[token][1] += 1\n",
    "        #print(word_vector)\n",
    "        trainingSet += [word_vector]\n",
    "    #El training set que se retorna contiene una columna mas debido al BIAS\n",
    "    trainingSet = np.array(trainingSet)\n",
    "    trainingSet = np.c_[np.ones(trainingSet_length),trainingSet]\n",
    "    return trainingSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0]\n",
      " [1]\n",
      " [1]\n",
      " ..., \n",
      " [0]\n",
      " [0]\n",
      " [1]]\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "##DEFINIMOS NUESTRA FUNCIÓN LOGISTICA(SIGMOIDE)\n",
    "def sigmoid(gamma):\n",
    "    if gamma < 0:\n",
    "        return 1 - 1 / (1 + math.exp(gamma))\n",
    "    return 1 / (1 + math.exp(-gamma))\n",
    "\n",
    "sigmoid = np.vectorize(sigmoid,otypes=[np.float])\n",
    "\n",
    "## Obteniendo los valores de los test\n",
    "b= np.array([df['sentiment']])\n",
    "\n",
    "b = b.T ## REPRESENTA A LOS VALORES TARGET\n",
    "print(b)\n",
    "\n",
    "\n",
    "## Inicializamos los pesos en forma aleatoria\n",
    "import random\n",
    "\n",
    "\n",
    "weights = np.full((number_features+1, 1), 0.0)\n",
    "for i in range( number_features+1 ):\n",
    "    weights[i] = round(random.uniform(-1,1),2)\n",
    "\n",
    "#sigmoid(np.dot(get_vectors(0,50,number_features),weights))\n",
    "\n",
    "# Haciendo la regresion para la función logística\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empezando el Entrenamiento\n",
      "Completado al 0.0 %\n",
      "Completado al 2.22 %\n",
      "Completado al 4.44 %\n",
      "Completado al 6.66 %\n",
      "Completado al 8.88 %\n",
      "Completado al 11.100000000000001 %\n",
      "Completado al 13.32 %\n",
      "Completado al 15.540000000000001 %\n",
      "Completado al 17.76 %\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-207-523431142e89>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mstop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;31m#print(start,stop)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mvectors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_vectors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnumber_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0;31m#print(vectors.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mtmp_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvectors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-97-3628abad19e9>\u001b[0m in \u001b[0;36mget_vectors\u001b[0;34m(_start, _stop, number_features)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mngrams\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_tmp\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkeys\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m                     \u001b[0;31m#print(token)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/nltk/util.py\u001b[0m in \u001b[0;36mngrams\u001b[0;34m(sequence, n, pad_left, pad_right, left_pad_symbol, right_pad_symbol)\u001b[0m\n\u001b[1;32m    468\u001b[0m         \u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msequence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    469\u001b[0m         \u001b[0mn\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 470\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msequence\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    471\u001b[0m         \u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    472\u001b[0m         \u001b[0;32myield\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "## Implementamos el estocastic gradient ascent\n",
    "## Evaluamos hasta los primeros 45000 ejemplos\n",
    "print(\"Empezando el Entrenamiento\")\n",
    "alpha = 0.0001\n",
    "for batch in range(45):\n",
    "    start = 1000*batch\n",
    "    stop = 1000*(batch+1)\n",
    "    #print(start,stop)\n",
    "    vectors = get_vectors(start,stop,number_features)\n",
    "    #print(vectors.shape)\n",
    "    tmp_result = sigmoid(np.dot(vectors,weights))\n",
    "    #print(tmp_result)\n",
    "    for i in range(start,stop):\n",
    "        for w in range(number_features + 1):\n",
    "            weights[w] = weights[w] + alpha * (b[i][0]-tmp_result[i%1000][0]) * vectors[i%1000][w]\n",
    "        #print(weights[0:5])\n",
    "    print(\"Completado al\",round(100/ 45,2) *batch, \"%\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.73689584]\n",
      " [ 0.70176302]\n",
      " [ 0.00107816]\n",
      " [ 0.31747291]\n",
      " [ 0.12355011]] [[1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]]\n",
      "0.529055795133\n"
     ]
    }
   ],
   "source": [
    "## testeamos el accuracy\n",
    "\n",
    "test_result = sigmoid(np.dot(get_vectors(45000,50000,number_features),weights))\n",
    "print(test_result[:5],b[45000:45000+5])\n",
    "\n",
    "acc = 0.0\n",
    "for i in range(0,5000):\n",
    "    acc += abs(test_result[i][0] - b[45000+i][0])\n",
    "print(acc/5000)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
