{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EJERCICIO N 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En esta primera prueba, intentamos extraer un feature vector para cada review. Este es un vector de 300 características, consistiendo de\n",
    "\n",
    "80 -> 1 gramas\n",
    "\n",
    "70 -> 2 gramas\n",
    "\n",
    "60 -> 3 gramas\n",
    "\n",
    "50 -> 4 gramas\n",
    "\n",
    "40 -> 5 gramas\n",
    "\n",
    "\n",
    "300 Features de los ngramas mas frecuentes en toda la base de datos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Titulo: Multilayer Neural Network\n",
    "Autor: M. Portocarrero\n",
    "\n",
    "RED NEURAL PARA CLASIFICACION BINARIA\n",
    "\n",
    "Recordemos que el objetivo de esta red es la de resolver un problema de clasificacion de dos clases\n",
    "Por ello, sólo tendremos una neurona en la Output Layer\n",
    "\n",
    "La arquitectura de esta red neural es como sigue\n",
    "n: numero de datos\n",
    "Input Layer:\n",
    "    j características\n",
    "Hidden Layer:\n",
    "    un numero x de neuronas\n",
    "Output Layer:\n",
    "    una sola neurona, con función Sigmoide\n",
    "\"\"\"\n",
    "\"\"\" <--- IMPORTS ---> \"\"\"\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\"\"\" <--- FUNCTIONS ---> \"\"\"\n",
    "def sigmoid(x,derivative = False):\n",
    "    if derivative:\n",
    "        return sigmoid(x)*(1 - sigmoid(x))\n",
    "    return 1.0 / (1.0 + np.exp(-x))\n",
    "\n",
    "def sigmoid_prime(x):\n",
    "    \"\"\" Funcion para calcular la derivada del sigmoide cuando x es sig(x)\"\"\"\n",
    "    return x * (1 - x)\n",
    "\n",
    "def evalPrediction(x,threshold,target):\n",
    "    \"\"\"Funcion que evalua el acierto de una salida, Devuelve un booleano\"\"\"\n",
    "    if x > threshold:\n",
    "        return target == 1\n",
    "    else:\n",
    "        return target == 0\n",
    "        \n",
    "sig_PrimeVectorized = np.vectorize(sigmoid_prime)\n",
    "sigVectorized = np.vectorize(sigmoid)\n",
    "evalPredictionVectorized = np.vectorize(evalPrediction)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\" <--- OBJECTS ---> \"\"\"\n",
    "class Weights(object):\n",
    "    def __init__(self, numFeatures,numNeurons):\n",
    "        self.numFeatures =  numFeatures\n",
    "        self.numNeurons = numNeurons\n",
    "        self.weights = np.random.random((numFeatures,numNeurons))\n",
    "    \n",
    "    def printWeights(self):\n",
    "        print(self.weights)\n",
    "\n",
    "class DeepNeuralNetwork(object):\n",
    "    \"\"\"\n",
    "    Red Neural que reune las capas y las matrices de pesos\n",
    "    data : Matriz con todos los datos de entrenamiento\n",
    "            Filas -> cada dato de entrenamiento, Cols: los coeficientes\n",
    "    target : Matriz donde se almacenan los resultados\n",
    "    \"\"\"\n",
    "    def __init__(self,neurons,data,target,_test_data,_test_target):\n",
    "        # We include the bias at the start of data matrix\n",
    "        self.training_data = data\n",
    "        self.n = data.shape[0] #Numero de ejemplos en el training\n",
    "        self. numFeatures = self.training_data.shape[1]\n",
    "        self.training_target = target\n",
    "        # Input Layer\n",
    "        self.inLayer = None\n",
    "        # First Weight Matrix(nro col = nro de neuronas)\n",
    "        self.neuronsInHiddenLayer = neurons\n",
    "        # acordemonos que un feature adicional seria el bias\n",
    "        self.weights_1 = np.random.random((self.numFeatures+1,self.neuronsInHiddenLayer))\n",
    "        # Hidden Layers\n",
    "        self.hiddenLayer = None        \n",
    "        # Second Weight Matrix, Agregar una columna para el bias\n",
    "        self.weights_2 = np.random.random((self.neuronsInHiddenLayer + 1,1)) # Solo una neurona de salida\n",
    "        # Output Layer\n",
    "        self.outLayer = None\n",
    "        self.Layers = None\n",
    "        \n",
    "        # Loading Test Data, (Agregamos una fila de 1 para el bias)\n",
    "        _test_data = np.append( np.full((len(_test_data),1),1),_test_data,axis = 1)\n",
    "        self.test_data = _test_data\n",
    "        self.test_target = _test_target\n",
    "        \n",
    "        ## Labels para imprimir los arreglos\n",
    "        self.enum = {\n",
    "                0:\"Input Data:\\n\",\n",
    "                1:\"Weights 1:\\n\",\n",
    "                2:\"Hidden Layer:\\n\",\n",
    "                3:\"Weights 2:\\n\",\n",
    "                4:\"Output Layer:\\n\",}\n",
    "        \n",
    "        \n",
    "    def forwardPropagation(self,i):\n",
    "        # Seleccionamos la cantidad de datos\n",
    "        self.inLayer = np.array([self.training_data[i%self.n]])\n",
    "        #self.inLayer = np.array(self.training_data)\n",
    "        self.inLayer = np.append( np.full((len(self.inLayer),1),1),self.inLayer,axis = 1)\n",
    "        \n",
    "        self.hiddenLayer = np.dot(self.inLayer,self.weights_1)\n",
    "        self.hiddenLayer = sigmoid(self.hiddenLayer)\n",
    "        self.hiddenLayer = np.append( np.full((len(self.hiddenLayer),1),1),self.hiddenLayer,axis = 1)\n",
    "        \n",
    "        self.outLayer = np.dot(self.hiddenLayer,self.weights_2)\n",
    "        self.outLayer = sigmoid(self.outLayer)\n",
    "        \n",
    "        self.updateLayers()\n",
    "        \n",
    "    def updateLayers(self):\n",
    "        self.Layers = [self.inLayer,self.weights_1,self.hiddenLayer,self.weights_2,self.outLayer]\n",
    "        \n",
    "    def backPropagation(self,i,learning_rate):\n",
    "        \"\"\"Back Propagation for Stochastic\"\"\"\n",
    "        error = (self.outLayer[0][0] - self.training_target[i%self.n])\n",
    "        weights_1 = self.weights_1\n",
    "        weights_2 = self.weights_2\n",
    "        \n",
    "        # First Layer Propagation\n",
    "        weights_2 = weights_2 - learning_rate * error * self.hiddenLayer.T\n",
    "        \n",
    "        # Second Layer Propagation\n",
    "        #Expresiones temporales\n",
    "        #Derivada del sigmoide = sig x * (1-sig x)\n",
    "        hidden = self.hiddenLayer.T \n",
    "        hidden = np.delete(hidden,(0),axis = 0)\n",
    "        hidden = sig_PrimeVectorized(hidden).T\n",
    "    \n",
    "        derivative = error * np.dot(np.array([self.inLayer[0]]).T,hidden)\n",
    "\n",
    "        w2 = self.weights_2\n",
    "        w2 = np.delete(w2,(0),axis=0)\n",
    "        w2 = np.tile(w2,self.neuronsInHiddenLayer)\n",
    "        derivative = np.dot(derivative,w2)\n",
    "        #print(derivative)\n",
    "\n",
    "        weights_1 = weights_1 - learning_rate * derivative\n",
    "        \n",
    "        \n",
    "        self.weights_1 = weights_1\n",
    "        self.weights_2 = weights_2\n",
    "        self.updateLayers()\n",
    "        \n",
    "        \n",
    "        \n",
    "    def Train(self,iterations,learn_rate):\n",
    "        \"\"\"\n",
    "        Function that executes the training that is one entire cycle\n",
    "        Adjust the weights matrices\n",
    "        \"\"\"\n",
    "        alpha = learn_rate\n",
    "        b= True\n",
    "        for i in range(iterations):\n",
    "            self.forwardPropagation(i)\n",
    "            self.backPropagation(i,alpha)\n",
    "            #self.printNN()\n",
    "            if(i % 10000 == 0):\n",
    "                print(\"Progress:\",i / 10000,\"%\")\n",
    "                if(self.getAccuracy() > 70 and b):\n",
    "                    alpha /= 10\n",
    "                    b = False\n",
    "        self.printNN()\n",
    "        \n",
    "    def printNN(self):\n",
    "        print(\"=================\")\n",
    "        i = 0\n",
    "        for layer in self.Layers:\n",
    "            print(self.enum[i],layer)\n",
    "            i +=1\n",
    "        print(\"=================\")\n",
    "    def getAccuracy(self):\n",
    "        \"\"\"\n",
    "        Funcion que imprime el accuracy con todos los datos de test\n",
    "        \"\"\"\n",
    "        prediction = np.dot(self.test_data,self.weights_1)\n",
    "        prediction = sigmoid(prediction)\n",
    "        prediction = np.append( np.full((len(prediction),1),1),prediction,axis = 1)\n",
    "        prediction = np.dot(prediction,self.weights_2)\n",
    "        prediction = sigmoid(prediction)\n",
    "        \n",
    "        threshold = 0.5\n",
    "    \n",
    "        #print(prediction)\n",
    "        \n",
    "        xs = evalPredictionVectorized(prediction,threshold,self.test_target)\n",
    "        #print(xs)\n",
    "        \n",
    "        numSuccesses = np.count_nonzero(xs == True)\n",
    "\n",
    "        \n",
    "        print(\"Acc:\", numSuccesses / len(xs) * 100)\n",
    "        return numSuccesses / len(xs) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I have seen this movie and in all honestly was...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I only saw this recently but had been aware of...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Albert Pyun delivers a very good action/drama ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>This is a truly great and beautiful movie. The...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>You know, I'm sure the boys were sitting aroun...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  sentiment\n",
       "0  I have seen this movie and in all honestly was...          0\n",
       "1  I only saw this recently but had been aware of...          1\n",
       "2  Albert Pyun delivers a very good action/drama ...          1\n",
       "3  This is a truly great and beautiful movie. The...          1\n",
       "4  You know, I'm sure the boys were sitting aroun...          0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('shuffled_movie_data.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop = stopwords.words('english')\n",
    "porter = PorterStemmer()\n",
    "\n",
    "# Devuelve una lista de tokens filtrados por stopwords conservando la secuencia\n",
    "def tokenizer(text):\n",
    "    text = re.sub('<[^>]*>', '', text)\n",
    "    emoticons = re.findall('(?::|;|=)(?:-)?(?:\\)|\\(|D|P)', text.lower())\n",
    "    text = re.sub('[\\W]+', ' ', text.lower()) + ' '.join(emoticons).replace('-', '')\n",
    "    #print(text.split())\n",
    "    text = [w for w in text.split() if w not in stop]\n",
    "    tokenized = [porter.stem(w) for w in text]\n",
    "    return text\n",
    "\n",
    "#tokenizer(df.iloc[1]['review'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing Opinions\n"
     ]
    }
   ],
   "source": [
    "from nltk.util import ngrams\n",
    "from nltk import FreqDist\n",
    "\n",
    "n = 50000\n",
    "print('Tokenizing Opinions')\n",
    "corpus_words = []\n",
    "for i in range(n):\n",
    "    corpus_words += [tokenizer(df.iloc[i]['review'])]\n",
    "#print(corpus_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing Opinions\n",
      "Adding up all values\n",
      "Completed  20.0 %\n",
      "Completed  40.0 %\n",
      "Completed  60.0 %\n",
      "Completed  80.0 %\n"
     ]
    }
   ],
   "source": [
    "#Generamos un diccionario con las palabras mas frecuentes\n",
    "#para ngramas n = 1,2,3,4,5\n",
    "word_dict = {}\n",
    "for size in 1,2,3,4,5:\n",
    "    word_dict[size] =  FreqDist(ngrams(corpus_words[0],size))\n",
    "\n",
    "print(\"Adding up all values\")\n",
    "for i in range(1,n):\n",
    "    for size in 1,2,3,4,5:\n",
    "        word_dict[size].update(FreqDist(ngrams(corpus_words[i],size)))\n",
    "    if i %10000 == 0:\n",
    "        print(\"Completed \",20*i/10000,\"%\")  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#word_dict[3].most_common(2)[0][0]\n",
    "\n",
    "## Aca debemos contar la ocurrencia de los n gramas obtenidos y guardarlos\n",
    "## para formar un vector para cada dato de ejemplo\n",
    "\n",
    "number_features = 0\n",
    "keys = []\n",
    "for i in 1,2,3,4,5:\n",
    "    number_features+= 90-10*i\n",
    "    tmp = word_dict[i].most_common(90-10*i)\n",
    "    for j in range(90-10*i):\n",
    "        keys += (tmp[j][0],)\n",
    "#keys\n",
    "#number_features = 300!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progreso:  0.0 %\n",
      "Progreso:  1.0 %\n",
      "Progreso:  2.0 %\n",
      "Progreso:  3.0 %\n",
      "Progreso:  4.0 %\n",
      "Progreso:  5.0 %\n",
      "Progreso:  6.0 %\n",
      "Progreso:  7.0 %\n",
      "Progreso:  8.0 %\n",
      "Progreso:  9.0 %\n",
      "Progreso:  10.0 %\n",
      "Progreso:  11.0 %\n",
      "Progreso:  12.0 %\n",
      "Progreso:  13.0 %\n",
      "Progreso:  14.0 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/maxito911/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:22: DeprecationWarning: generator 'ngrams' raised StopIteration\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progreso:  15.0 %\n",
      "Progreso:  16.0 %\n",
      "Progreso:  17.0 %\n",
      "Progreso:  18.0 %\n",
      "Progreso:  19.0 %\n",
      "Progreso:  20.0 %\n",
      "Progreso:  21.0 %\n",
      "Progreso:  22.0 %\n",
      "Progreso:  23.0 %\n",
      "Progreso:  24.0 %\n",
      "Progreso:  25.0 %\n",
      "Progreso:  26.0 %\n",
      "Progreso:  27.0 %\n",
      "Progreso:  28.0 %\n",
      "Progreso:  29.0 %\n",
      "Progreso:  30.0 %\n",
      "Progreso:  31.0 %\n",
      "Progreso:  32.0 %\n",
      "Progreso:  33.0 %\n",
      "Progreso:  34.0 %\n",
      "Progreso:  35.0 %\n",
      "Progreso:  36.0 %\n",
      "Progreso:  37.0 %\n",
      "Progreso:  38.0 %\n",
      "Progreso:  39.0 %\n",
      "Progreso:  40.0 %\n",
      "Progreso:  41.0 %\n",
      "Progreso:  42.0 %\n",
      "Progreso:  43.0 %\n",
      "Progreso:  44.0 %\n",
      "Progreso:  45.0 %\n",
      "Progreso:  46.0 %\n",
      "Progreso:  47.0 %\n",
      "Progreso:  48.0 %\n",
      "Progreso:  49.0 %\n",
      "Finished\n"
     ]
    }
   ],
   "source": [
    "#print(keys)\n",
    "##Dictionario que almacena los indices para cada palabra en la lista keys\n",
    "text_tmp_dict = {}\n",
    "for i in range(number_features):\n",
    "    text_tmp_dict[keys[i]] = i\n",
    "#print(text_tmp_dict)\n",
    "\n",
    "## La funcion cuenta el numero de apariciones de los tokens para cada valor de entrenamiento\n",
    "## y los almacena en una array\n",
    "def get_vectors(start,stop,number_features):\n",
    "    trainingSet_length = stop - start\n",
    "    trainingSet = []\n",
    "    for example in range(start,stop):\n",
    "        text_tmp = corpus_words[example]\n",
    "        #print(text_tmp)\n",
    "        #print(\"Occurency in Example\")\n",
    "\n",
    "        word_vector = [0 for i in range(number_features)]\n",
    "        #print(word_vector)\n",
    "\n",
    "        for i in 1,2,3,4,5:\n",
    "            for token in ngrams(text_tmp,i):\n",
    "                if token in keys:\n",
    "                    #print(token)\n",
    "                    word_vector[text_tmp_dict[token]] += 1\n",
    "                    #text_tmp_dict[token][1] += 1\n",
    "        #print(word_vector)\n",
    "        trainingSet += [word_vector]\n",
    "        if(example % 1000 == 0):\n",
    "            print(\"Progreso: \", example /1000,\"%\")\n",
    "        \n",
    "    #El training set que se retorna contiene una columna mas debido al BIAS\n",
    "    trainingSet = np.array(trainingSet)\n",
    "    #trainingSet = np.c_[np.ones(trainingSet_length),trainingSet]\n",
    "    print(\"Finished\")\n",
    "    return trainingSet\n",
    "\n",
    "data = get_vectors(0,50000,number_features)\n",
    "#print(data, data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]] (50000, 1)\n"
     ]
    }
   ],
   "source": [
    "## Obtenemos los valores target\n",
    "## Obteniendo los valores de los test\n",
    "target = np.array([df['sentiment']])\n",
    "\n",
    "target = target.T ## REPRESENTA A LOS VALORES TARGET\n",
    "print(target[:5],target.shape) # Verificamos correspondencia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Separamos la data 45k para el training y 5k para el target\n",
    "training_data = data[:45000]\n",
    "training_target = target[:45000]\n",
    "\n",
    "test_data = data[45000:]\n",
    "test_target = target[45000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El accuracy de clasificacion alcanzado llega a un maximo de 75% sobre 5k muestras de test.\n",
    "Se trato de subir el accuracy sobre entrenando en las muestras de training pero no se tuvo mejora.\n",
    "Tal vez aumentando un poco mas el numero de entrenamientos\n",
    "\n",
    "NOTA: EL learning rate es dinámico, es decir, se reduce a medida que se va alcanzando mayor accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 0.0 %\n",
      "Acc: 50.44\n",
      "Progress: 1.0 %\n",
      "Acc: 50.44\n",
      "Progress: 2.0 %\n",
      "Acc: 49.559999999999995\n",
      "Progress: 3.0 %\n",
      "Acc: 50.339999999999996\n",
      "Progress: 4.0 %\n",
      "Acc: 49.559999999999995\n",
      "Progress: 5.0 %\n",
      "Acc: 49.559999999999995\n",
      "Progress: 6.0 %\n",
      "Acc: 50.44\n",
      "Progress: 7.0 %\n",
      "Acc: 49.66\n",
      "Progress: 8.0 %\n",
      "Acc: 50.44\n",
      "Progress: 9.0 %\n",
      "Acc: 50.42\n",
      "Progress: 10.0 %\n",
      "Acc: 50.44\n",
      "Progress: 11.0 %\n",
      "Acc: 49.8\n",
      "Progress: 12.0 %\n",
      "Acc: 50.54\n",
      "Progress: 13.0 %\n",
      "Acc: 49.68\n",
      "Progress: 14.0 %\n",
      "Acc: 49.559999999999995\n",
      "Progress: 15.0 %\n",
      "Acc: 60.0\n",
      "Progress: 16.0 %\n",
      "Acc: 65.12\n",
      "Progress: 17.0 %\n",
      "Acc: 70.44\n",
      "Progress: 18.0 %\n",
      "Acc: 71.88\n",
      "Progress: 19.0 %\n",
      "Acc: 72.46000000000001\n",
      "Progress: 20.0 %\n",
      "Acc: 72.68\n",
      "Progress: 21.0 %\n",
      "Acc: 72.56\n",
      "Progress: 22.0 %\n",
      "Acc: 72.42\n",
      "Progress: 23.0 %\n",
      "Acc: 72.88\n",
      "Progress: 24.0 %\n",
      "Acc: 72.92\n",
      "Progress: 25.0 %\n",
      "Acc: 73.2\n",
      "Progress: 26.0 %\n",
      "Acc: 73.04\n",
      "Progress: 27.0 %\n",
      "Acc: 73.06\n",
      "Progress: 28.0 %\n",
      "Acc: 73.2\n",
      "Progress: 29.0 %\n",
      "Acc: 73.22\n",
      "Progress: 30.0 %\n",
      "Acc: 73.54\n",
      "Progress: 31.0 %\n",
      "Acc: 73.3\n",
      "Progress: 32.0 %\n",
      "Acc: 73.32\n",
      "Progress: 33.0 %\n",
      "Acc: 73.66\n",
      "Progress: 34.0 %\n",
      "Acc: 73.76\n",
      "Progress: 35.0 %\n",
      "Acc: 73.54\n",
      "Progress: 36.0 %\n",
      "Acc: 73.76\n",
      "Progress: 37.0 %\n",
      "Acc: 73.74000000000001\n",
      "Progress: 38.0 %\n",
      "Acc: 73.76\n",
      "Progress: 39.0 %\n",
      "Acc: 73.76\n",
      "Progress: 40.0 %\n",
      "Acc: 73.92\n",
      "Progress: 41.0 %\n",
      "Acc: 73.66\n",
      "Progress: 42.0 %\n",
      "Acc: 73.74000000000001\n",
      "Progress: 43.0 %\n",
      "Acc: 73.98\n",
      "Progress: 44.0 %\n",
      "Acc: 73.94\n",
      "Progress: 45.0 %\n",
      "Acc: 73.8\n",
      "Progress: 46.0 %\n",
      "Acc: 74.08\n",
      "Progress: 47.0 %\n",
      "Acc: 74.03999999999999\n",
      "Progress: 48.0 %\n",
      "Acc: 74.03999999999999\n",
      "Progress: 49.0 %\n",
      "Acc: 74.11999999999999\n",
      "Progress: 50.0 %\n",
      "Acc: 73.8\n",
      "Progress: 51.0 %\n",
      "Acc: 74.1\n",
      "Progress: 52.0 %\n",
      "Acc: 74.06\n",
      "Progress: 53.0 %\n",
      "Acc: 74.22\n",
      "Progress: 54.0 %\n",
      "Acc: 73.98\n",
      "Progress: 55.0 %\n",
      "Acc: 74.18\n",
      "Progress: 56.0 %\n",
      "Acc: 73.96000000000001\n",
      "Progress: 57.0 %\n",
      "Acc: 74.06\n",
      "Progress: 58.0 %\n",
      "Acc: 74.26\n",
      "Progress: 59.0 %\n",
      "Acc: 73.96000000000001\n",
      "Progress: 60.0 %\n",
      "Acc: 74.18\n",
      "Progress: 61.0 %\n",
      "Acc: 74.26\n",
      "Progress: 62.0 %\n",
      "Acc: 74.38\n",
      "Progress: 63.0 %\n",
      "Acc: 74.18\n",
      "Progress: 64.0 %\n",
      "Acc: 74.16\n",
      "Progress: 65.0 %\n",
      "Acc: 74.14\n",
      "Progress: 66.0 %\n",
      "Acc: 74.26\n",
      "Progress: 67.0 %\n",
      "Acc: 74.33999999999999\n",
      "Progress: 68.0 %\n",
      "Acc: 74.3\n",
      "Progress: 69.0 %\n",
      "Acc: 74.42\n",
      "Progress: 70.0 %\n",
      "Acc: 74.24\n",
      "Progress: 71.0 %\n",
      "Acc: 74.48\n",
      "Progress: 72.0 %\n",
      "Acc: 74.44\n",
      "Progress: 73.0 %\n",
      "Acc: 74.5\n",
      "Progress: 74.0 %\n",
      "Acc: 74.36\n",
      "Progress: 75.0 %\n",
      "Acc: 74.53999999999999\n",
      "Progress: 76.0 %\n",
      "Acc: 74.46000000000001\n",
      "Progress: 77.0 %\n",
      "Acc: 74.5\n",
      "Progress: 78.0 %\n",
      "Acc: 74.52\n",
      "Progress: 79.0 %\n",
      "Acc: 74.44\n",
      "Progress: 80.0 %\n",
      "Acc: 74.76\n",
      "Progress: 81.0 %\n",
      "Acc: 74.6\n",
      "Progress: 82.0 %\n",
      "Acc: 74.62\n",
      "Progress: 83.0 %\n",
      "Acc: 74.53999999999999\n",
      "Progress: 84.0 %\n",
      "Acc: 74.52\n",
      "Progress: 85.0 %\n",
      "Acc: 74.66000000000001\n",
      "Progress: 86.0 %\n",
      "Acc: 74.4\n",
      "Progress: 87.0 %\n",
      "Acc: 74.68\n",
      "Progress: 88.0 %\n",
      "Acc: 74.83999999999999\n",
      "Progress: 89.0 %\n",
      "Acc: 74.96000000000001\n",
      "Progress: 90.0 %\n",
      "Acc: 74.8\n",
      "Progress: 91.0 %\n",
      "Acc: 74.74\n",
      "Progress: 92.0 %\n",
      "Acc: 74.68\n",
      "Progress: 93.0 %\n",
      "Acc: 74.7\n",
      "Progress: 94.0 %\n",
      "Acc: 74.68\n",
      "Progress: 95.0 %\n",
      "Acc: 74.62\n",
      "Progress: 96.0 %\n",
      "Acc: 74.82\n",
      "Progress: 97.0 %\n",
      "Acc: 74.86\n",
      "Progress: 98.0 %\n",
      "Acc: 75.06\n",
      "Progress: 99.0 %\n",
      "Acc: 74.98\n",
      "=================\n",
      "Input Data:\n",
      " [[1 1 0 1 0 0 0 0 0 1 0 1 0 1 0 0 0 2 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 1 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0]]\n",
      "Weights 1:\n",
      " [[-0.38897009 -0.51931435 -0.00210579 ... -0.27275863 -0.23605967\n",
      "  -0.42881084]\n",
      " [-0.64950725 -0.79443585 -0.34117477 ... -0.36866447 -0.72363281\n",
      "  -0.15659892]\n",
      " [-0.43004544 -0.3754643   0.14269693 ...  0.05781287 -0.43792403\n",
      "   0.32127609]\n",
      " ...\n",
      " [ 0.00597872  0.92018358  0.67431028 ...  0.5029662   0.7266948\n",
      "   0.17550091]\n",
      " [ 0.94068662  0.05400482  0.69164823 ...  0.85771197  0.39553987\n",
      "   0.83896561]\n",
      " [ 0.45772777  0.4602399   0.05666424 ...  0.39429587  0.44895662\n",
      "   0.72097857]]\n",
      "Hidden Layer:\n",
      " [[1.         0.55973609 0.96390324 0.94719889 0.94976142 0.95063921\n",
      "  0.86258433 0.91152052 0.91474126 0.75765791 0.81955613 0.96236681\n",
      "  0.86856631 0.62274383 0.96671027 0.68081814 0.93549728 0.77739186\n",
      "  0.80328081 0.90198302 0.91421026 0.97574025 0.92378118 0.88813907\n",
      "  0.77013666 0.98693664 0.97964667 0.9118129  0.97447937 0.97584969\n",
      "  0.57669421 0.9291171  0.93281222]]\n",
      "Weights 2:\n",
      " [[-2.98787934]\n",
      " [ 0.11425732]\n",
      " [ 0.00700466]\n",
      " [ 0.22049188]\n",
      " [ 0.1784615 ]\n",
      " [ 0.23960408]\n",
      " [ 0.83053678]\n",
      " [ 0.69803285]\n",
      " [ 0.20668856]\n",
      " [ 0.02744156]\n",
      " [-0.25553569]\n",
      " [-0.25249165]\n",
      " [ 0.24821953]\n",
      " [-0.16475646]\n",
      " [ 0.36143168]\n",
      " [-0.01033822]\n",
      " [ 0.78993021]\n",
      " [-0.10334236]\n",
      " [ 0.52309697]\n",
      " [ 0.10142218]\n",
      " [ 0.00379775]\n",
      " [ 0.14295371]\n",
      " [ 0.36989799]\n",
      " [ 0.25575106]\n",
      " [ 0.1505877 ]\n",
      " [ 0.62760013]\n",
      " [ 0.84047878]\n",
      " [-0.25967011]\n",
      " [ 0.04006   ]\n",
      " [ 0.32672768]\n",
      " [-0.15342019]\n",
      " [-0.11543853]\n",
      " [ 0.21254646]]\n",
      "Output Layer:\n",
      " [[0.94361301]]\n",
      "=================\n",
      "Acc: 74.98\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "74.98"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NN = DeepNeuralNetwork(32,training_data,training_target,test_data,test_target)\n",
    "NN.Train(1000000,0.01)\n",
    "NN.getAccuracy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EJERCICIO N° 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En esta parte realizaremos el mismo ejercicio pero usando word embedding \n",
    "(word2vec). El metodo es sencillo, se calculara el embedding para cada review, realizando el promedio de la suma de los embeddings de cada palabra encontrada en el preview.\n",
    "NOTA: Acordemonos que usamos palabras prefiltradas por stopwords, en el tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "model = Word2Vec(corpus_words,size=100)\n",
    "#print(model)\n",
    "w2v = dict(zip(model.wv.index2word,model.wv.syn0))\n",
    "#print(w2v)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 100)\n",
      "[[0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]] (50000, 1)\n"
     ]
    }
   ],
   "source": [
    "def MeanEmbeddingVectorizer(tokenized_review):\n",
    "    mean_vector = []\n",
    "    for word in tokenized_review:\n",
    "        if word in w2v:\n",
    "            mean_vector.append(w2v[word].tolist())\n",
    "        else:\n",
    "            mean_vector.append(np.zeros(100))\n",
    "    mean_vector = np.mean(mean_vector,axis = 0)\n",
    "    #print(mean_vector)\n",
    "    return mean_vector\n",
    "\n",
    "data = []\n",
    "for i in range(50000):\n",
    "    data.append(MeanEmbeddingVectorizer(corpus_words[i]))\n",
    "data = np.array(data)\n",
    "print(data.shape)\n",
    "\n",
    "## Obtenemos los valores target\n",
    "## Obteniendo los valores de los test\n",
    "target = np.array([df['sentiment']])\n",
    "\n",
    "target = target.T ## REPRESENTA A LOS VALORES TARGET\n",
    "print(target[:5],target.shape) # Verificamos correspondencia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = data[:45000]\n",
    "training_target = target[:45000]\n",
    "\n",
    "test_data = data[45000:]\n",
    "test_target = target[45000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 0.0 %\n",
      "Acc: 50.44\n",
      "Progress: 1.0 %\n",
      "Acc: 57.720000000000006\n",
      "Progress: 2.0 %\n",
      "Acc: 61.040000000000006\n",
      "Progress: 3.0 %\n",
      "Acc: 65.36\n",
      "Progress: 4.0 %\n",
      "Acc: 69.94\n",
      "Progress: 5.0 %\n",
      "Acc: 73.7\n",
      "Progress: 6.0 %\n",
      "Acc: 74.5\n",
      "Progress: 7.0 %\n",
      "Acc: 74.96000000000001\n",
      "Progress: 8.0 %\n",
      "Acc: 75.18\n",
      "Progress: 9.0 %\n",
      "Acc: 75.56\n",
      "Progress: 10.0 %\n",
      "Acc: 75.9\n",
      "Progress: 11.0 %\n",
      "Acc: 76.16000000000001\n",
      "Progress: 12.0 %\n",
      "Acc: 76.7\n",
      "Progress: 13.0 %\n",
      "Acc: 76.92\n",
      "Progress: 14.0 %\n",
      "Acc: 76.98\n",
      "Progress: 15.0 %\n",
      "Acc: 77.58\n",
      "Progress: 16.0 %\n",
      "Acc: 78.03999999999999\n",
      "Progress: 17.0 %\n",
      "Acc: 78.14\n",
      "Progress: 18.0 %\n",
      "Acc: 78.46\n",
      "Progress: 19.0 %\n",
      "Acc: 78.74\n",
      "Progress: 20.0 %\n",
      "Acc: 78.58000000000001\n",
      "Progress: 21.0 %\n",
      "Acc: 79.0\n",
      "Progress: 22.0 %\n",
      "Acc: 78.86\n",
      "Progress: 23.0 %\n",
      "Acc: 78.97999999999999\n",
      "Progress: 24.0 %\n",
      "Acc: 79.47999999999999\n",
      "Progress: 25.0 %\n",
      "Acc: 79.78\n",
      "Progress: 26.0 %\n",
      "Acc: 79.82000000000001\n",
      "Progress: 27.0 %\n",
      "Acc: 79.9\n",
      "Progress: 28.0 %\n",
      "Acc: 79.96\n",
      "Progress: 29.0 %\n",
      "Acc: 79.96\n",
      "Progress: 30.0 %\n",
      "Acc: 80.16\n",
      "Progress: 31.0 %\n",
      "Acc: 80.02\n",
      "Progress: 32.0 %\n",
      "Acc: 80.12\n",
      "Progress: 33.0 %\n",
      "Acc: 80.42\n",
      "Progress: 34.0 %\n",
      "Acc: 80.46\n",
      "Progress: 35.0 %\n",
      "Acc: 80.64\n",
      "Progress: 36.0 %\n",
      "Acc: 80.74\n",
      "Progress: 37.0 %\n",
      "Acc: 80.94\n",
      "Progress: 38.0 %\n",
      "Acc: 81.02000000000001\n",
      "Progress: 39.0 %\n",
      "Acc: 81.24\n",
      "Progress: 40.0 %\n",
      "Acc: 81.14\n",
      "Progress: 41.0 %\n",
      "Acc: 81.14\n",
      "Progress: 42.0 %\n",
      "Acc: 81.3\n",
      "Progress: 43.0 %\n",
      "Acc: 81.42\n",
      "Progress: 44.0 %\n",
      "Acc: 81.39999999999999\n",
      "Progress: 45.0 %\n",
      "Acc: 81.5\n",
      "Progress: 46.0 %\n",
      "Acc: 81.54\n",
      "Progress: 47.0 %\n",
      "Acc: 81.66\n",
      "Progress: 48.0 %\n",
      "Acc: 81.64\n",
      "Progress: 49.0 %\n",
      "Acc: 81.72\n",
      "Progress: 50.0 %\n",
      "Acc: 81.8\n",
      "Progress: 51.0 %\n",
      "Acc: 81.82000000000001\n",
      "Progress: 52.0 %\n",
      "Acc: 81.86\n",
      "Progress: 53.0 %\n",
      "Acc: 82.0\n",
      "Progress: 54.0 %\n",
      "Acc: 82.08\n",
      "Progress: 55.0 %\n",
      "Acc: 82.14\n",
      "Progress: 56.0 %\n",
      "Acc: 82.22\n",
      "Progress: 57.0 %\n",
      "Acc: 82.26\n",
      "Progress: 58.0 %\n",
      "Acc: 82.3\n",
      "Progress: 59.0 %\n",
      "Acc: 82.39999999999999\n",
      "Progress: 60.0 %\n",
      "Acc: 82.48\n",
      "Progress: 61.0 %\n",
      "Acc: 82.69999999999999\n",
      "Progress: 62.0 %\n",
      "Acc: 82.66\n",
      "Progress: 63.0 %\n",
      "Acc: 82.62\n",
      "Progress: 64.0 %\n",
      "Acc: 82.66\n",
      "Progress: 65.0 %\n",
      "Acc: 82.76\n",
      "Progress: 66.0 %\n",
      "Acc: 82.72\n",
      "Progress: 67.0 %\n",
      "Acc: 82.88\n",
      "Progress: 68.0 %\n",
      "Acc: 82.92\n",
      "Progress: 69.0 %\n",
      "Acc: 82.92\n",
      "Progress: 70.0 %\n",
      "Acc: 83.14\n",
      "Progress: 71.0 %\n",
      "Acc: 83.14\n",
      "Progress: 72.0 %\n",
      "Acc: 83.1\n",
      "Progress: 73.0 %\n",
      "Acc: 83.2\n",
      "Progress: 74.0 %\n",
      "Acc: 83.26\n",
      "Progress: 75.0 %\n",
      "Acc: 83.32000000000001\n",
      "Progress: 76.0 %\n",
      "Acc: 83.3\n",
      "Progress: 77.0 %\n",
      "Acc: 83.38\n",
      "Progress: 78.0 %\n",
      "Acc: 83.54\n",
      "Progress: 79.0 %\n",
      "Acc: 83.54\n",
      "Progress: 80.0 %\n",
      "Acc: 83.64\n",
      "Progress: 81.0 %\n",
      "Acc: 83.7\n",
      "Progress: 82.0 %\n",
      "Acc: 83.74000000000001\n",
      "Progress: 83.0 %\n",
      "Acc: 83.8\n",
      "Progress: 84.0 %\n",
      "Acc: 83.86\n",
      "Progress: 85.0 %\n",
      "Acc: 83.76\n",
      "Progress: 86.0 %\n",
      "Acc: 83.82\n",
      "Progress: 87.0 %\n",
      "Acc: 84.0\n",
      "Progress: 88.0 %\n",
      "Acc: 84.02\n",
      "Progress: 89.0 %\n",
      "Acc: 84.08\n",
      "Progress: 90.0 %\n",
      "Acc: 84.17999999999999\n",
      "Progress: 91.0 %\n",
      "Acc: 84.24000000000001\n",
      "Progress: 92.0 %\n",
      "Acc: 84.24000000000001\n",
      "Progress: 93.0 %\n",
      "Acc: 84.3\n",
      "Progress: 94.0 %\n",
      "Acc: 84.34\n",
      "Progress: 95.0 %\n",
      "Acc: 84.36\n",
      "Progress: 96.0 %\n",
      "Acc: 84.42\n",
      "Progress: 97.0 %\n",
      "Acc: 84.24000000000001\n",
      "Progress: 98.0 %\n",
      "Acc: 84.38\n",
      "Progress: 99.0 %\n",
      "Acc: 84.44\n",
      "=================\n",
      "Input Data:\n",
      " [[ 1.         -0.20274855  0.5995823  -0.48176002  0.29863467 -0.01342496\n",
      "  -0.2939018  -0.46497934  0.13637211  0.11595747  0.14395893  0.33944514\n",
      "  -0.448464   -0.13342889 -0.49814333 -0.37676046 -0.15968323  0.4395935\n",
      "   0.81297208 -0.08944687 -0.33592369 -0.09190789  0.6703653   0.21167679\n",
      "   0.20909551  0.33946167 -0.56537254 -0.01546343 -0.37861396  0.50633556\n",
      "  -0.18376988  0.71489376  0.03254785 -0.24033412  0.59361835  0.68185746\n",
      "  -0.42459438  0.08003674  0.50987378  0.04840411 -0.30874879 -0.10837882\n",
      "  -0.04496731  0.29534734 -0.35641327  0.25530494  0.32260459  0.46317127\n",
      "  -0.00634365  0.24494543 -0.38538906 -0.24545882  0.83059186  0.29696774\n",
      "  -0.06278962  0.02903453 -0.1418646  -0.33743106  0.99337306 -0.06672151\n",
      "  -0.30164169 -0.64113012  0.05731909 -0.44129976 -0.37767332  0.05790735\n",
      "   0.65548546 -0.41457256  0.21485791 -0.0932608   0.10058261 -0.48538065\n",
      "  -0.15133668  0.19500465 -0.01720699 -0.0736793  -0.3235009   0.07890158\n",
      "   0.39663706  0.18015636  1.06022546 -0.39913765 -0.77121083 -0.62458626\n",
      "  -0.01618889  0.33571919 -0.28734733  0.61556976  0.48241636 -0.10577701\n",
      "   0.25205835 -0.05368107  0.27128407 -0.52724495  0.31797693  0.68230733\n",
      "  -0.01542259  0.93641773 -0.36986185  0.28922948 -0.78897313]]\n",
      "Weights 1:\n",
      " [[-0.54409261 -0.52267999 -0.5188312  ... -0.79312128 -0.18682193\n",
      "  -0.29155404]\n",
      " [ 0.86637492  0.85437116  0.34868991 ...  0.43766639  0.33823198\n",
      "   0.76268398]\n",
      " [ 0.14963899  0.07234024  0.19927368 ...  0.26829843 -0.05430616\n",
      "  -0.39639185]\n",
      " ...\n",
      " [ 0.24001404  0.06606844  0.01481705 ...  0.16317897 -0.66779695\n",
      "  -0.63756607]\n",
      " [-0.73574044 -0.47676258 -0.67440154 ... -0.74681408 -0.66558498\n",
      "  -0.05690138]\n",
      " [ 0.11967052 -0.12823164  0.24893573 ... -0.33509127  0.2707233\n",
      "   0.04445495]]\n",
      "Hidden Layer:\n",
      " [[1.         0.97735058 0.96616308 0.97904306 0.99448709 0.99490599\n",
      "  0.99121872 0.97655525 0.99699987 0.99834082 0.99463194 0.8814298\n",
      "  0.99336143 0.99520516 0.97964498 0.99109017 0.99539091 0.98106079\n",
      "  0.99492297 0.98979714 0.99665618 0.97652225 0.99929556 0.9939029\n",
      "  0.98612937 0.96570245 0.99338527 0.98567707 0.98202403 0.98247291\n",
      "  0.99708787 0.98888963 0.99307212]]\n",
      "Weights 2:\n",
      " [[-3.00690753e+00]\n",
      " [-2.55316135e-02]\n",
      " [ 9.67745680e-02]\n",
      " [ 4.55768556e-01]\n",
      " [ 8.39326473e-01]\n",
      " [ 5.42325154e-01]\n",
      " [ 6.73836177e-01]\n",
      " [ 5.59916422e-01]\n",
      " [ 1.39264057e-01]\n",
      " [ 1.38950586e-01]\n",
      " [ 4.59847726e-01]\n",
      " [ 4.48160538e-01]\n",
      " [ 3.90557800e-01]\n",
      " [ 2.45946682e-01]\n",
      " [-3.67029473e-02]\n",
      " [-2.77801405e-03]\n",
      " [-3.27860546e-01]\n",
      " [ 1.24894560e-01]\n",
      " [ 2.85001453e-01]\n",
      " [ 1.02516845e-01]\n",
      " [ 5.44168950e-01]\n",
      " [ 6.91027258e-01]\n",
      " [ 1.23033293e-01]\n",
      " [ 3.20421354e-01]\n",
      " [ 4.95106383e-01]\n",
      " [ 4.42004052e-01]\n",
      " [ 6.26341866e-01]\n",
      " [-1.96449511e-01]\n",
      " [-5.02664457e-03]\n",
      " [ 6.66435912e-01]\n",
      " [ 1.37411425e-01]\n",
      " [ 3.03922200e-01]\n",
      " [ 2.68261260e-01]]\n",
      "Output Layer:\n",
      " [[0.99827018]]\n",
      "=================\n",
      "Acc: 84.48\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "84.48"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NN = DeepNeuralNetwork(32,training_data,training_target,test_data,test_target)\n",
    "NN.Train(1000000,0.001)\n",
    "NN.getAccuracy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para el segundo caso se nota una gran mejora. El accuracy llego hasta 85%. Notemos que los embeddings se generaron para la base de datos.\n",
    "\n",
    "Conclusiones:\n",
    "\n",
    "Es importante usar las mejores características.\n",
    "\n",
    "Los embeddings entrenados son una gran herramienta a tener en cuenta en Sentiment Analisis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
